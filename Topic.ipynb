{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0da7ab4e-b8f9-4d5f-a66d-14affb324dc4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
      "Collecting tokenizers>=0.10.3\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.59.0)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.10.0-cp38-cp38-win_amd64.whl (226.6 MB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.11.1-cp38-cp38-win_amd64.whl (984 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.20.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.24.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.6.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from sentence_transformers) (3.6.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.7.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.4.4)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.4.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (7.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
      "Requirement already satisfied: six in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from torchvision->sentence_transformers) (8.2.0)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=120999 sha256=05ddd9fd77b8bf39e9e581322a7a0dbe6d1a1011a6e6b6049de8e278b2fc895d\n",
      "  Stored in directory: c:\\users\\eason.chen\\appdata\\local\\pip\\cache\\wheels\\52\\19\\88\\6625593382e23a926740e6fcee0f2df0a0de25766094842a28\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: torch, tokenizers, sacremoses, huggingface-hub, transformers, torchvision, sentencepiece, sentence-transformers\n",
      "Successfully installed huggingface-hub-0.2.1 sacremoses-0.0.46 sentence-transformers-2.1.0 sentencepiece-0.1.96 tokenizers-0.10.3 torch-1.10.0 torchvision-0.11.1 transformers-4.12.5\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "64223ed4-36ab-4134-bcea-a9a95481e2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bryson Research Group Computational Systems Biology Overview Teaching Suggested Projects Biography Publications Computational Systems Biology Our lab applies computational and mathematical techniques to understand biological systems that generally have clinical relevance. One area of research is modelling bacteria at the atomic', \" molecular and population levels. Bacteria are relatively simple and so we can attempt to gain a mechanistic-level understanding of their working. Gaining such an understanding is clinicially important in terms of finding new approaches to tackle drug-resistant strains. A different area of our research looks at more complex systems where gaining a mechanistic understanding is not viable. In particular we are interested in neurodegenerative diseases such as Alzheimer's\", \" Parkinson's and Huntington's disease. We approach understanding these complex systems by integrating different types of high-throughput data and applying a variety of machine learning and network-based approaches to search for patterns in the data that could suggest different disease mechanisms. Again we try to integrate data both at the molecular level within neurons (e.g. genomics and transcriptomics data) and also at the population level (i.e. whole brain using MRI and EEG data). Modelling the quorum mechanism of Streptococcus pneumonia The aim of this project is to understand changes in the population level behaviour of Streptococcus pneumonia (such as competence\", ' fractricide and biofilm formation) from changes occurring at the molecular level within the quorum signalling pathway. To accomplish this we are integrating genomic data', ' cis-regulatory motif', ' microarray', ' ODE-based modelling of the signalling and gene regulation pathways and agent-based modelling approaches. This is a collaborative project with Dr Bambos Charalambous in Department of Infection. Understanding the ER/mitochondrial intra-cellular signalling pathways in human Signalling between the ER and mitochondria under stress conditions are important mechanisms within cancer and neurodegeration - leading to either adaptation or apoptosis. The aim of this project is to determine pathways involved in these processes by applying machine learning to public microarray data over cancer cell lines. Any tentative pathways will then be experimentally confirmed within cell cultures. This is a collaborative CoMPLEX project with Dr Gyorgy Szabadkai in Cell & Development Biology. Modelling the purine metabolic pathway in human. Purine metabolism is implicated in a variety of cancers such as acute myeloid cancer where drug treatments which inhibit IMPDH', ' one of the key purine metabolic enzymes', ' have been shown to stop cell proliferation. The aim of this project is to develop ODE models', ' parameterized using literature and RNASeq data', ' to predict the overall metabolic effects of inhibiting particular enzymes', ' potentially revealing other drug targets within the network. This is a collaborative CoMPLEX project with Dr Geraint Thomas in Cell & Development Biology. Department of Computer Science University College London Gower Street London WC1 6BT']\n"
     ]
    }
   ],
   "source": [
    "my_file = open(\"text.txt\", \"r\")\n",
    "content = my_file.read()\n",
    "content_list = content.split(\",\")\n",
    "my_file.close()\n",
    "print(content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5682c987-6757-4789-bef0-a3c4daf05f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "embeddings = model.encode(content_list)\n",
    "#embeddings = embeddings.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5afc9fa8-daff-49d5-9277-9e338ff27af8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting umap\n",
      "  Downloading umap-0.1.1.tar.gz (3.2 kB)\n",
      "Building wheels for collected packages: umap\n",
      "  Building wheel for umap (setup.py): started\n",
      "  Building wheel for umap (setup.py): finished with status 'done'\n",
      "  Created wheel for umap: filename=umap-0.1.1-py3-none-any.whl size=3564 sha256=3ddd3ccd4bf3cd3175c4cce49b3211725427479e5ef5cd7c29383dcc6a564f85\n",
      "  Stored in directory: c:\\users\\eason.chen\\appdata\\local\\pip\\cache\\wheels\\d4\\13\\91\\2e752dc8dab5df027854bd33d2b65e1dc5cdc107fd1133990f\n",
      "Successfully built umap\n",
      "Installing collected packages: umap\n",
      "Successfully installed umap-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce8bd21a-8366-43aa-a3fb-164167907ec0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting umap-learn\n",
      "  Downloading umap-learn-0.5.2.tar.gz (86 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from umap-learn) (1.20.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from umap-learn) (0.24.1)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from umap-learn) (1.6.2)\n",
      "Requirement already satisfied: numba>=0.49 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from umap-learn) (0.53.1)\n",
      "Collecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.5.tar.gz (1.1 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from umap-learn) (4.59.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn) (52.0.0.post20210125)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn) (0.36.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from pynndescent>=0.5->umap-learn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22->umap-learn) (2.1.0)\n",
      "Building wheels for collected packages: umap-learn, pynndescent\n",
      "  Building wheel for umap-learn (setup.py): started\n",
      "  Building wheel for umap-learn (setup.py): finished with status 'done'\n",
      "  Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82696 sha256=966531d9fb9e45c9c22690a25edce060801d8502c51f8af8071c8ad7c0ac478d\n",
      "  Stored in directory: c:\\users\\eason.chen\\appdata\\local\\pip\\cache\\wheels\\f2\\64\\75\\df601da9514261c8cb0830b9515d2b94b5a51f09ddeae92b9e\n",
      "  Building wheel for pynndescent (setup.py): started\n",
      "  Building wheel for pynndescent (setup.py): finished with status 'done'\n",
      "  Created wheel for pynndescent: filename=pynndescent-0.5.5-py3-none-any.whl size=52587 sha256=b6d625b2cf7983a845a7256807f88b36f9af9465a41ca0655d009cbb947ed6ff\n",
      "  Stored in directory: c:\\users\\eason.chen\\appdata\\local\\pip\\cache\\wheels\\e4\\0e\\b5\\07c0c231aacb04e5d1046fe7459bb27ea79f95b5edbe88e435\n",
      "Successfully built umap-learn pynndescent\n",
      "Installing collected packages: pynndescent, umap-learn\n",
      "Successfully installed pynndescent-0.5.5 umap-learn-0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5224c8fd-d2b2-4eed-aee6-c55361744a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a10bcd29-92bb-42e1-ac59-b8ad55f0408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embeddings = umap.UMAP(n_neighbors=2, \n",
    "                            n_components=1, \n",
    "                            metric='cosine').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1334e1e7-f457-460a-98ee-4f40f01e3520",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hdbscan in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (0.8.27)\n",
      "Requirement already satisfied: cython>=0.27 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from hdbscan) (0.29.23)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from hdbscan) (1.6.2)\n",
      "Requirement already satisfied: six in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from hdbscan) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from hdbscan) (1.20.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from hdbscan) (0.24.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from hdbscan) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\eason.chen\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20->hdbscan) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b4e6e20-322d-4ab9-89c0-bbcaa03728a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import hdbscan\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf28a4d6-f3e7-4a4f-bc6a-d38921821c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eason.chen\\Anaconda3\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data\n",
    "umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "result['labels'] = cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6525dfd5-9215-48c1-9f59-74b5a645c890",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.DataFrame(content_list, columns=[\"Doc\"])\n",
    "docs_df['Topic'] = cluster.labels_\n",
    "docs_df['Doc_ID'] = range(len(docs_df))\n",
    "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cea1e981-f0d6-4011-8a5e-3aba163cd20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "  \n",
    "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e2b81706-e2da-4b2e-847c-f50aec1d88ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Size\n",
       "0     -1    12"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .Doc\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "912fa753-8cd0-4f95-8416-a854817bb169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 0.22105663849357624),\n",
       " ('project', 0.17193515657377895),\n",
       " ('level', 0.14652345295965485),\n",
       " ('understanding', 0.14652345295965485),\n",
       " ('cell', 0.14652345295965485),\n",
       " ('systems', 0.14652345295965485),\n",
       " ('modelling', 0.14652345295965485),\n",
       " ('pathways', 0.12039518302700086),\n",
       " ('complex', 0.12039518302700086),\n",
       " ('signalling', 0.12039518302700086)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words[-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2169746-4111-4606-966f-21ca78134b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
